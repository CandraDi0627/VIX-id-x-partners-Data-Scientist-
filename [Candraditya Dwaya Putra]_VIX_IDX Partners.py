# -*- coding: utf-8 -*-
"""Copy of Final Task PBI ID/X Partners.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1uuPBOEkWOLnxp11-3tDSufuNeSS3GRFY

# **Connect to Google Drive**
"""

#Mount Drive
from google.colab import drive
drive.mount('/content/drive/')

"""# **Libraries**"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
pd.set_option('display.max_rows', 500)
pd.set_option('display.max_columns', 500)
pd.set_option('display.width', 1000)
import warnings
warnings.simplefilter(action='ignore', category=FutureWarning)
warnings.filterwarnings("ignore", category=DeprecationWarning)
warnings.filterwarnings("ignore")
from sklearn.preprocessing import OneHotEncoder

"""## **Import Data**"""

df = pd.read_csv('/content/drive/MyDrive/Rakamin/PBI/ID X Partners/dataset/loan_data_2007_2014.csv')
df.head(5)

"""##**Data Understanding**"""

df.info()

"""# **Data Explore**"""

full_non_null = [col for col in df.columns if df[col].isnull().all()]
print(full_non_null)
print(len(full_non_null),"column(s)")

"""Terdapat 17 Kolom Null yang selanjutnya akan di drop"""

df = df.drop(axis=1, columns=full_non_null)
df.info()

percent_missing = df.isnull().sum() * 100 / len(df)
dtypes=[df[col].dtype for col in df.columns]
missing_value_df = pd.DataFrame({'data_type':dtypes,
                                 'percent_missing': percent_missing})
missing_value_df.sort_values('percent_missing', ascending=False, inplace=True)
missing_value_df.head(10)

"""Nilai yang hilang > 50%:

* **`mths_since_last_record`** = Jumlah bulan sejak pencatatan publik terakhir
* **`mths_since_last_major_derog`** = Bulan sejak rating 90 hari terakhir atau lebih buruk
* **`desc`** = Deskripsi pinjaman yang diberikan oleh peminjam
* **`mths_since_last_delinq`** = Jumlah bulan sejak tunggakan terakhir peminjam

Nilai yang hilang 40% - 50%:

* **`next_Payment_d`** = Pembayaran bulan lalu telah diterima

Nilai yang hilang 1% - 20%

* **`tot_cur_bal`** = Total saldo semua akun saat ini
* **`tot_coll_amt`** = Total jumlah penagihan yang terutang
* **`total_rev_hi_lim`**= Total kredit/batas kredit tertinggi yang bergulir
* **`emp_title`** = Jabatan yang diberikan oleh Peminjam saat mengajukan pinjaman.
* **`emp_length`** = Lama bekerja dalam tahun. Nilai yang mungkin adalah antara 0 dan 10 dimana 0 berarti kurang dari satu tahun dan 10 berarti sepuluh tahun atau lebih.
"""

missing_value_df.tail(48)

"""Gambaran missing Value dibawah 1% berada pada 13 kolom teratas

## **Check Duplicated Data**
"""

df.duplicated().sum()

"""Tidak terdapat duplicated data

# **Data Preparation**

## **Handling Missing Value**
"""

df = df[df.columns[~df.columns.isin(['mths_since_last_record','desc','next_pymnt_d'])]]
df.info()

df.head(5)

"""## **`Imputation`**


---

* Untuk **`mths_since_last_major_derog`** dan **`mths_since_last_delinq`** (nilai yang hilang di atas 50%), saya memperhitungkannya dengan nilai "0" (nol)
* Untuk yang lain, saya menggunakan nilai mediannya untuk fitur numerik dan mode untuk fitur kategorikal
"""

for col in ['mths_since_last_major_derog','mths_since_last_delinq']:
    df[col] = df[col].fillna(0)

df[['mths_since_last_major_derog','mths_since_last_delinq']].isnull().sum()

# Numerical columns
for col in df.select_dtypes(exclude='object'):
    df[col] = df[col].fillna(df[col].median())
df.isnull().sum()

# Non numerical columns
for col in df.select_dtypes(include='object'):
    df[col] = df[col].fillna(df[col].mode().iloc[0])
print("Updated Missing Values")
df.isnull().sum()

df.head(5)

"""# **`Unique Values`**"""

print("Unique Features (Numerical)")
print(df.select_dtypes(exclude='object').nunique())

"""**`Unnamed : 0`**, **`id`** dan **`member_id`** memiliki unique value di setiap baris. sedangkan **`policy_code`** memiliki 1 unique value"""

print("Unique Features (Categorical)")
print(df.select_dtypes(exclude=['int','float']).nunique())

"""**`emp_title`**, **`url`**,**`title`**, **`zip_code`**, **`earliest_cr_line`** memiliki lebih dari 500 unique values
**`application_type `** hanya memiliki 1 unique values
"""

df["term"].unique()

"""diperlukan menghapus spasi yang mungkin ada di awal atau akhir setiap nilai dalam kolom. Hal ini diperlukan ketika ingin memastikan bahwa data bersih dan konsisten sebelum melakukan analisis lebih lanjut atau pemrosesan data."""

def word_strip(x):
  return x.strip()

df['term'] = df['term'].apply(lambda x: word_strip(x))
df["term"].unique()

"""## **`Cek spasi setiap feature`**"""

df["grade"].unique()

df["sub_grade"].unique()

df["emp_length"].unique()

df["home_ownership"].unique()

df["verification_status"].unique()

df["purpose"].unique()

"""# **`Formatting Target Variable`**"""

# loan_menjadi target karena terdapat informasi pembayaran konsumen
df["loan_status"].unique()

"""Berdasarkan variabel loan_status memiliki beberapa nilai, yaitu:

* `Current`artinya pembayaran lancar
* `Charged Off` artinya pembayaran macet
sehingga dihapusbukukan
* `Late` artinya pembayaran telat dilakukan; In Grace Period artinya dalam masa tenggang
* `Fully Paid` artinya pembayaran lunas; Default artinya pembayaran macet

Berdasarkan status loan diatas, maka dapat diberikan label *bad borrower* (Peminjam yang baik) dan *good borrower* (Peminjam yang baik). Menurut slik OJK, terdapat rincian skor kredit berdasarkan BI Checking/Slik OJK:
* Skor 1: Kredit Lancar, artinya debitur selalu memenuhi kewajibannya untuk membayar cicilan setiap bulan beserta bunganya hingga lunas tanpa pernah menunggak.
* Skor 2: Kredit DPK atau Kredit dalam Perhatian Khusus, artinya debitur tercatat menunggak cicilan kredit 1-90 hari
* Skor 3: Kredit Tidak Lancar, artinya debitur tercatat menunggak cicilan kredit 91-120 hari
* Skor 4: Kredit Diragukan, artinya debitur tercatat menunggak cicilan kredit 121-180 hari
* Skor 5: Kredit Macet, artinya debitur tercatat menunggak cicilan kredit lebih 180 hari.

Berdasarkan pengertian diatas, maka pada project ini akan dilakukan labeling sebagai berikut:
* Good Borrower (1) :**` Fully Paid`**, Does not meet the credit policy. Status:Fully Paid
* Bad Borrower (0) : **`Charged Off`**, Does not meet the credit policy. Status:**`Charged Off`**, **`Default`**, **`Late (31-120 days)`**
Undetachable Loan (-1) : **`Current`**, **`In Grace Period`**, **`Late (16-30 days)`**

Saya akan menggunakan Good Borrower (1) dan Bad Borrower (0) nanti untuk klasifikasi biner
Nantinya kolom Undetachable Loan (-1) akan dihilangkan karena masih merupakan pinjaman dalam proses yang belum dapat dideteksi baik atau buruknya.
"""

# encoding target variable
target_dict = {'Fully Paid':1,
               'Does not meet the credit policy. Status:Fully Paid':1,
               'Charged Off':0,
               'Does not meet the credit policy. Status:Charged Off':0,
               'Default':0,
               'Late (31-120 days)':0,
               'Current':-1,
               'In Grace Period':-1,
               'Late (16-30 days)':-1}
# Buat nilai yang dipetakan di kolom baru
df['loan_status'] = df['loan_status'].map(target_dict)
# Review dataset
df.head()

df = df.loc[~df['loan_status'].isin([-1])].reset_index(drop=True)
df.info()

df.tail()

"""## **`Feature Enginering : Datetime`**"""

# Bulan dimana batas kredit peminjam yang paling awal dilaporkan dibuka
df['earliest_cr_line'].value_counts()

df['earliest_cr_line'] = pd.to_datetime(df['earliest_cr_line'], format='%b-%y')

# Bulan terakhir LC menarik kredit untuk pinjaman ini
df['last_credit_pull_d'].value_counts()

# Pembayaran Bulan lalu yang telah diterima
df['last_pymnt_d'].value_counts()

# Bulan Pendanaan
df['issue_d'].value_counts()

df[['issue_d','last_pymnt_d','last_credit_pull_d']].head(5)

def date_time(dt):
  if dt.year > 2016:
    dt = dt.replace(year=dt.year-100)
  return dt

# Set standard datetime
df['earliest_cr_line'] = pd.to_datetime(df['earliest_cr_line'], format='%b-%y') # Bulan dimana batas kredit peminjam yang paling awal dilaporkan dibuka
df['earliest_cr_line'] = df['earliest_cr_line'].apply(lambda x: date_time(x))
df['issue_d'] = pd.to_datetime(df['issue_d'], format='%b-%y') # Bulan Pendanaan
df['last_pymnt_d'] = pd.to_datetime(df['last_pymnt_d'],format='%b-%y') # Pembayaran Bulan lalu yang telah diterima
df['last_credit_pull_d'] = pd.to_datetime(df['last_credit_pull_d'],format='%b-%y') # Bulan terakhir LC menarik kredit untuk pinjaman ini
df[['earliest_cr_line','issue_d','last_pymnt_d','last_credit_pull_d']].head(5)

"""dibutuhkan kolom baru untuk datetime:

* **`pymnt_time`** = jumlah bulan antara pinjaman yang didanai (**`issue_d`**) dan pembayaran terakhir diterima (**`last_pymnt_d`**)
* **`credit_pull_year`** = jumlah tahun antara batas kredit peminjam yang dilaporkan paling awal dibuka (**`earliest_cr_line`**) dan LC terbaru yang menarik kredit untuk pinjaman ini (**`last_credit_pull_d`**)
"""

def diff_month(d1, d2):
    return (d1.year - d2.year) * 12 + d1.month - d2.month

def diff_year(d1, d2):
    return (d1.year - d2.year)

((df.apply(lambda x: diff_month(x.last_pymnt_d, x.issue_d), axis=1) < 0)).any().any()

((df.apply(lambda x: diff_month(x.last_credit_pull_d, x.earliest_cr_line), axis=1) < 0)).any().any()

df['pymnt_time'] = df.apply(lambda x: diff_month(x.last_pymnt_d, x.issue_d), axis=1)
df['credit_pull_year'] = df.apply(lambda x: diff_year(x.last_credit_pull_d, x.earliest_cr_line), axis=1)
print('Adding features succeed')

df.info()

df.head(5)

df.to_csv('df_clean.csv', index=False)
!cp 'df_clean.csv' '/content/drive/MyDrive/Rakamin/PBI/ID X Partners/dataset'
print('Saving cleaned data is done!')

"""Data sudah bersih dan dapat digunakan Exploratory Data Analysis untuk mengekstaksi insight. data yang digunakan adalah **`df_clean.csv`**"""

df_clean = pd.read_csv('/content/drive/MyDrive/Rakamin/PBI/ID X Partners/dataset/df_clean.csv')
df_clean.head(5)

"""# **Descriptive Statistic**
**Numerical Feature**
"""

df_clean.describe()

"""**Categorical Feature**"""

df_clean.describe(exclude=['int','float'])

"""**Analysing Distribution Plot**"""

df_clean.dtypes.value_counts()

non_used = ['Unnamed: 0','id','member_id','policy_code', 'loan_status']
uni_dist = df_clean.select_dtypes(include=[np.float64,np.int64])
uni_dist = uni_dist[uni_dist.columns[~uni_dist.columns.isin(non_used)]]

plt.figure(figsize=(40, 20))
for i in range(0, 11):
    plt.subplot(11, 3, i+1)
    sns.distplot(uni_dist.iloc[:,i], color='red')
    plt.tight_layout()

plt.figure(figsize=(40, 20))
for i in range(11, 22):
    plt.subplot(11, 3, i+1)
    sns.distplot(uni_dist.iloc[:,i], color='blue')
    plt.tight_layout()

plt.figure(figsize=(40, 20))
for i in range(22, 33):
    plt.subplot(12, 3, i+1)
    sns.distplot(uni_dist.iloc[:,i], color='red')
    plt.tight_layout()

"""* didominasi feature skewed
* hanya 5 feature yang tidak skewed, yaitu **`loan_amnt`**, **`int_rate`**, **`dti`**, **`funded_amnt`**, dan **`funded_amnt_ivn`**.

## **Outlier Check**
"""

plt.figure(figsize=(40, 20))
for i in range(0, 11):
    plt.subplot(11, 3, i+1)
    sns.boxplot(uni_dist.iloc[:,i], color='gray',orient='h')
    plt.tight_layout()

plt.figure(figsize=(40, 20))
for i in range(11, 22):
    plt.subplot(11, 3, i+1)
    sns.boxplot(uni_dist.iloc[:,i], color='gray',orient='h')
    plt.tight_layout()

plt.figure(figsize=(40, 20))
for i in range(22, 33):
    plt.subplot(12, 3, i+1)
    sns.boxplot(uni_dist.iloc[:,i], color='gray',orient='h')
    plt.tight_layout()

"""* didominasi feature yang memiliki outlier
* hanya 5 feature yang tidak memiliki outlier, yaitu **`loan_amnt`**,**`funded_amnt`**,**`funded_amnt_inv`**,**`int_rate`**.

# **Feature Selection : Correlation Analysis**
"""

non_used = ['Unnamed: 0','id','member_id','policy_code','loan_status']
uni_dist = df_clean.select_dtypes(include=[np.float64,np.int64])
uni_dist = uni_dist[uni_dist.columns[~uni_dist.columns.isin(non_used)]]
fig = plt.figure(figsize = (40,10))
sns.heatmap(uni_dist.corr(),cmap='coolwarm', annot = True);

def top_correlation (df_clean,n):
    corr_matrix = df_clean.corr()
    correlation = (corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))
                 .stack()
                 .sort_values(ascending=False))
    correlation = pd.DataFrame(correlation).reset_index()
    correlation.columns=["Variable_1","Variable_2","Correlation"]
    correlation = correlation.reindex(correlation.Correlation.abs().sort_values(ascending=False).index).reset_index().drop(["index"],axis=1)
    return correlation.head(n)
print("High Correlated Features (Corr > 0.5)")
top_correlation(uni_dist,39)

df_clean_corr = top_correlation(uni_dist,41)
df_clean_corr.to_excel('df_clean_corr_2.xlsx', index=False)
!cp 'df_clean_corr_2.xlsx' '/content/drive/MyDrive/Rakamin/PBI/ID X Partners/dataset/'
print('Saving correlation data is done!')

uni_dist.head(5)

# Menghapus feature yang memiliki lebih dari 500 unique value dan 1 unique value

removed_unused = ['Unnamed: 0','id','member_id','policy_code','emp_title','url','title','zip_code','earliest_cr_line']
multicol = ['last_credit_pull_d','last_pymnt_d','issue_d','addr_state','application_type',
            'out_prncp_inv','funded_amnt','total_pymnt_inv','funded_amnt_inv','total_rec_prncp','out_prncp',
            'revol_bal','total_pymnt','recoveries','total_rec_int','total_acc','loan_amnt']
removed_all = removed_unused + multicol

df_clean_a = df_clean[df_clean.columns[~df_clean.columns.isin(removed_all)]].reset_index(drop=True)
df_clean_b = df_clean[df_clean.columns[~df_clean.columns.isin(removed_unused)]].reset_index(drop=True)

df_clean_a.head(5)

df_clean_b.head(5)

df_clean_a.to_csv('df_clean_a.csv', index=False)
!cp 'df_clean_a.csv' '/content/drive/MyDrive/Rakamin/PBI/ID X Partners/dataset/'
print('Saving cleaned data is done!')

df_clean_b.to_csv('df_clean_b.csv', index=False)
!cp 'df_clean_b.csv' '/content/drive/MyDrive/Rakamin/PBI/ID X Partners/dataset/'
print('Saving cleaned data is done!')

"""## **Categorical Encoding**"""

df_clean_a = pd.read_csv('/content/drive/MyDrive/Rakamin/PBI/ID X Partners/dataset/df_clean_a.csv')
df_clean_a.head(5)

df_clean_a.info()

df_clean_b = pd.read_csv('/content/drive/MyDrive/Rakamin/PBI/ID X Partners/dataset/df_clean_b.csv')
df_clean_b.head(5)

df_clean_b.info()

df_clean_a["term"].unique()

def text_num(text):
  return [int(s) for s in text.split() if s.isdigit()][0]

sns.distplot(df_clean_a["term"].apply(lambda x: text_num(x)), color='red')
plt.tight_layout()
plt.show()

df_clean_a["term"] = df_clean_a["term"].apply(lambda x: text_num(x))
df_clean_a.head(5)

df_clean_a["grade"].unique()

# Define a dictionary for encoding ordinal variable
target_dict = {'A':6,
               'B':5,
               'C':4,
               'D':3,
               'E':2,
               'F':1,
               'G':0}
# Create the mapped values in a new column
df_clean_a["grade"] = df_clean_a["grade"].map(target_dict)

# Review dataset
df_clean_a.head(5)

df_clean_a["sub_grade"].unique()

def f_A(row):
    if row == 'A1':
        val = 1
    elif row == 'A2':
        val = 2
    elif row == 'A3':
        val = 3
    elif row == 'A4':
        val = 4
    elif row == 'A5':
        val = 5
    else:
        val = 0
    return val

def f_B(row):
    if row == 'B1':
        val = 1
    elif row == 'B2':
        val = 2
    elif row == 'B3':
        val = 3
    elif row == 'B4':
        val = 4
    elif row == 'B5':
        val = 5
    else:
        val = 0
    return val

def f_C(row):
    if row == 'C1':
        val = 1
    elif row == 'C2':
        val = 2
    elif row == 'C3':
        val = 3
    elif row == 'C4':
        val = 4
    elif row == 'C5':
        val = 5
    else:
        val = 0
    return val

def f_D(row):
    if row == 'D1':
        val = 1
    elif row == 'D2':
        val = 2
    elif row == 'D3':
        val = 3
    elif row == 'D4':
        val = 4
    elif row == 'D5':
        val = 5
    else:
        val = 0
    return val

def f_E(row):
    if row == 'E1':
        val = 1
    elif row == 'E2':
        val = 2
    elif row == 'E3':
        val = 3
    elif row == 'E4':
        val = 4
    elif row == 'E5':
        val = 5
    else:
        val = 0
    return val

def f_F(row):
    if row == 'F1':
        val = 1
    elif row == 'F2':
        val = 2
    elif row == 'F3':
        val = 3
    elif row == 'F4':
        val = 4
    elif row == 'F5':
        val = 5
    else:
        val = 0
    return val

def f_G(row):
    if row == 'G1':
        val = 1
    elif row == 'G2':
        val = 2
    elif row == 'G3':
        val = 3
    elif row == 'G4':
        val = 4
    elif row == 'G5':
        val = 5
    else:
        val = 0
    return val

df_clean_a['SubGrade_A'] = df_clean_a["sub_grade"].apply(f_A)
df_clean_a['SubGrade_B'] = df_clean_a["sub_grade"].apply(f_B)
df_clean_a['SubGrade_C'] = df_clean_a["sub_grade"].apply(f_C)
df_clean_a['SubGrade_D'] = df_clean_a["sub_grade"].apply(f_D)
df_clean_a['SubGrade_E'] = df_clean_a["sub_grade"].apply(f_E)
df_clean_a['SubGrade_F'] = df_clean_a["sub_grade"].apply(f_F)
df_clean_a['SubGrade_G'] = df_clean_a["sub_grade"].apply(f_G)
df_clean_a = df_clean_a.drop(axis=1, columns="sub_grade")

df_clean_a.head(5)

df_clean_a["emp_length"].unique()

# Define a dictionary for encoding ordinal variable
target_dict = {'< 1 year':0,
               '1 year':1,
               '2 years':2,
               '3 years':3,
               '4 years':4,
               '5 years':5,
               '6 years':6,
               '7 years':7,
               '8 years':8,
               '9 years':9,
               '10+ years':10}
# Create the mapped values in a new column
df_clean_a["emp_length"] = df_clean_a["emp_length"].map(target_dict)

df_clean_a.head()

df_clean_a["home_ownership"].unique()

df_clean_a["home_ownership"].value_counts()

"""ANY, OTHER, dan NONE akan digabungkan menjadi OTHER
Lalu akan digunakan One Hot Encoding untuk fitur ini
"""

# Define a dictionary for aggregating variable
target_dict = {'MORTGAGE':'MORTGAGE',
               'RENT':'RENT',
               'OWN':'OWN',
               'OTHER':'OTHER',
               'ANY':'OTHER',
               'NONE':'OTHER'}
# Create the mapped values in a new column
df_clean_a["home_ownership"] = df_clean_a["home_ownership"].map(target_dict)

df_clean_a.head(5)

encoder = OneHotEncoder(sparse=False)
df_clean_a_encoded = pd.DataFrame(encoder.fit_transform(df_clean_a[["home_ownership"]]))
df_clean_a_encoded.columns = encoder.get_feature_names_out(["home_ownership"])
df_clean_a = pd.concat([df_clean_a, df_clean_a_encoded], axis=1)
df_clean_a.drop(["home_ownership"] ,axis=1, inplace=True)
df_clean_a.head()

df_clean_a["verification_status"].unique()

df_clean_a["verification_status"].value_counts()

ncoder = OneHotEncoder(sparse=False)
df_clean_a_encoded = pd.DataFrame(encoder.fit_transform(df_clean_a[["verification_status"]]))
df_clean_a_encoded.columns = encoder.get_feature_names_out(["verification_status"])
df_clean_a = pd.concat([df_clean_a, df_clean_a_encoded], axis=1)
df_clean_a.drop(["verification_status"] ,axis=1, inplace=True)
df_clean_a.head()

df_clean_a['pymnt_plan'].unique()

# Define a dictionary for encoding ordinal variable
target_dict = {'n':0,
               'y':1}
# Create the mapped values in a new column
df_clean_a["pymnt_plan"] = df_clean_a["pymnt_plan"].map(target_dict)

df_clean_a.head(5)

df_clean_a["loan_status"].unique()

df_clean_a["purpose"].unique()

df_clean_a["purpose"].value_counts()

"""* **`home_improvement`**, **`car`**, **`medical`**, **`wedding`**, **`moving`**, **`house`**, **`vacation`**, **`educational`** dapat digabungkan menjadi **`private_use`** dikarenakan saling terkait yaitu pengeluaran pribadi.
**`renewable_energy`** dapat digabungkan menjadi **`other`**
"""

# Define a dictionary for aggregating variable
target_dict = {'debt_consolidation':'debt_consolidation',
               'credit_card':'credit_card',
               'home_improvement':'private_use',
               'other':'other',
               'major_purchase':'major_purchase',
               'small_business':'small_business',
               'car':'private_use',
               'medical':'private_use',
               'wedding':'private_use',
               'moving':'private_use',
               'house':'private_use',
               'vacation':'private_use',
               'educational':'private_use',
               'renewable_energy':'other'}
# Create the mapped values in a new column
df_clean_a["purpose"] = df_clean_a["purpose"].map(target_dict)

df_clean_a["purpose"].value_counts()

encoder = OneHotEncoder(sparse=False)
df_clean_a_encoded = pd.DataFrame(encoder.fit_transform(df_clean_a[["purpose"]]))
df_clean_a_encoded.columns = encoder.get_feature_names_out(["purpose"])
df_clean_a = pd.concat([df_clean_a, df_clean_a_encoded], axis=1)
df_clean_a.drop(["purpose"] ,axis=1, inplace=True)
df_clean_a.head()

df_clean_a["initial_list_status"].unique()

encoder = OneHotEncoder(sparse=False)
df_clean_a_encoded = pd.DataFrame(encoder.fit_transform(df_clean_a[["initial_list_status"]]))
df_clean_a_encoded.columns = encoder.get_feature_names_out(["initial_list_status"])
df_clean_a = pd.concat([df_clean_a, df_clean_a_encoded], axis=1)
df_clean_a.drop(["initial_list_status"] ,axis=1, inplace=True)
df_clean_a.head()

df_clean_a.info()

df_clean_a.to_csv('df_clean_a_prep_1.csv', index=False)
!cp 'df_clean_a_prep_1.csv' '/content/drive/MyDrive/Rakamin/PBI/ID X Partners/dataset/'
print('Saving data is done!')

"""## **Handling Outlier**"""

df_clean_aa = pd.read_csv('/content/drive/MyDrive/Rakamin/PBI/ID X Partners/dataset/df_clean_a_prep_1.csv')
df_clean_aa.head(5)

df_clean_aa['delinq_2yrs'].unique()

df_clean_aa['inq_last_6mths'].unique()

df_clean_aa['open_acc'].unique()

df_clean_aa['mths_since_last_major_derog'].unique()

len(df_clean_aa['mths_since_last_major_derog'].unique())

df_clean_aa['pub_rec'].unique()

"""Tidak termasuk numerical feature, karena memiliki Range"""

df_clean_aa['collections_12_mths_ex_med'].unique()

"""Tidak termasuk numerical feature, karena memiliki Range"""

df_clean_aa['acc_now_delinq'].unique()

"""Tidak termasuk numerical feature, karena memiliki Range"""

def subset_by_iqr(df, column):
    """Remove outliers from a dataframe by column, including optional
       whiskers, removing rows for which the column value are
       less than Q1-1.5IQR or greater than Q3+1.5IQR.
    Args:
        df (`:obj:pd.DataFrame`): A pandas dataframe to subset
        column (str): Name of the column to calculate the subset from.
        whisker_width (float): Optional, loosen the IQR filter by a
                               factor of `whisker_width` * IQR.
    Returns:
        (`:obj:pd.DataFrame`): Filtered dataframe
    """
    whisker_width=1.5
    # Calculate Q1, Q2 and IQR
    q1 = df[column].quantile(0.25)
    q3 = df[column].quantile(0.75)
    iqr = q3 - q1
    # Apply filter with respect to IQR, including optional whiskers
    filter = (df[column] >= q1 - whisker_width*iqr) & (df[column] <= q3 + whisker_width*iqr)
    return df.loc[filter].reset_index(drop=True)

"""Feature yang memiliki outlier yang parah :

* **`installment`**
* **`annual_inc`**
* **`open_acc`**
* **`total_rec_late_fee`**
* **`last_pymnt_amnt`**
* **`total_rev_hi_lim`**
* **`total_coll_amt`**
* **`collection_recovery_fee`**
* **`tot_cur_bal`**
* **`credit_pull_year`**
"""

df_clean_aa.head()

numerical = ['int_rate','installment','annual_inc','dti','delinq_2yrs','inq_last_6mths','mths_since_last_delinq','open_acc',
             'revol_util','total_rec_late_fee','collection_recovery_fee','last_pymnt_amnt','mths_since_last_major_derog','tot_coll_amt',
             'tot_cur_bal','total_rev_hi_lim','pymnt_time','credit_pull_year']

outlier = ['installment','annual_inc','open_acc','total_rec_late_fee','last_pymnt_amnt','total_rev_hi_lim',
           'tot_coll_amt','collection_recovery_fee','tot_cur_bal','pymnt_time','credit_pull_year']

# Example for whiskers = 1.5, as requested by the OP
print(f'Count of rows before removing outlier: {len(df_clean_aa)}')
for i in outlier:
  df_clean_aa_out = subset_by_iqr(df_clean_aa, i)
print(f'Count of rows after removing outlier: {len(df_clean_aa_out)}')

"""# **Training Test Split**

70% Training + 30% Testing
"""

# Pisahkan fitur dan variabel target (df_train)
df_train_feat = df_clean_aa_out.loc[:, df_clean_aa_out.columns != "loan_status"]
df_train_target = df_clean_aa_out["loan_status"]

df_train_feat.to_csv('df_train_feat.csv', index=False)
!cp 'df_train_feat.csv' '/content/drive/MyDrive/Rakamin/PBI/ID X Partners/dataset/'

df_train_target.to_csv('df_train_target.csv', index=False)
!cp 'df_train_target.csv' '/content/drive/MyDrive/Rakamin/PBI/ID X Partners/dataset/'
print('Saving data is done!')

from collections import Counter
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(df_train_feat, df_train_target, test_size=0.3,
                                                    random_state=42, stratify=df_train_target)
print('Class from training data df_train',Counter(y_train))

print('Class from testing data df_test',Counter(y_test))

# Distribusi of training target
plt.figure(figsize=(6,6))
plt.pie(
        y_train.value_counts(),
        autopct='%.2f',
        explode=[0.1,0],
        labels=["Yes","No"],
        shadow=True,
        textprops={'fontsize': 14},
        colors=["orange","red"],
        startangle=35)

plt.title("Proporsi Target Kelas",fontsize=20, fontweight='bold', pad=20)
plt.show()

"""# **Exploratory Data Analysis**"""

df_clean = pd.read_csv('/content/drive/MyDrive/Rakamin/PBI/ID X Partners/dataset/df_clean.csv')
df_clean.head(5)

from wordcloud import WordCloud
wordcloud = WordCloud().generate(' '.join(emp for emp in df_clean.emp_title))

plt.figure(figsize=(10,15))
plt.imshow(wordcloud)
plt.axis("off")
plt.show()

"""## **Bagaimana dengan mengklasifikasikan nilai peminjam kita dan status pinjamannya?**"""

plt.figure(figsize=(10,5))
grade_loan = df_clean.groupby(['grade', 'loan_status'])['id'].count().reset_index()
# plot with seaborn barplot
p = sns.barplot(data=grade_loan, x='grade', y='id', hue='loan_status',palette=['red','blue'])
plt.title("Grade and Loan Status")
legend_labels, _= p.get_legend_handles_labels()
p.legend(legend_labels,['Bad Borrower', 'Good Borrower'])
plt.show(p)

"""# **Modelling**"""

df_train_feat = pd.read_csv('/content/drive/MyDrive/Rakamin/PBI/ID X Partners/dataset/df_train_feat.csv')
df_train_feat.head(5)

df_train_target = pd.read_csv('/content/drive/MyDrive/Rakamin/PBI/ID X Partners/dataset/df_train_target.csv')
df_train_target['loan_status']

from collections import Counter
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(df_train_feat, df_train_target['loan_status'], test_size=0.3,
                                                    random_state=42, stratify=df_train_target)
print('Class from training data df_train',Counter(y_train))

print('Class from testing data df_test',Counter(y_test))

from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import StandardScaler
from imblearn.over_sampling import SMOTE
from imblearn.pipeline import Pipeline
from sklearn.preprocessing import FunctionTransformer
from imblearn.over_sampling import SMOTE

def log_transform(x):
  return np.log(x + 1)

# Pipeline to transform the numerical features
numerical = ['int_rate','installment','annual_inc','dti','delinq_2yrs','inq_last_6mths','mths_since_last_delinq','open_acc',
             'revol_util','total_rec_late_fee','collection_recovery_fee','last_pymnt_amnt','mths_since_last_major_derog','tot_coll_amt',
             'tot_cur_bal','total_rev_hi_lim','pymnt_time','credit_pull_year']
skewed = ['installment','annual_inc','delinq_2yrs','inq_last_6mths','mths_since_last_delinq','open_acc',
          'revol_util','total_rec_late_fee','collection_recovery_fee','last_pymnt_amnt','mths_since_last_major_derog','tot_coll_amt',
          'tot_cur_bal','total_rev_hi_lim','pymnt_time','credit_pull_year']
diff = list(set(numerical) - set(skewed))

smt = SMOTE(random_state=42)
ss = StandardScaler()
log_transformer = FunctionTransformer(log_transform) # remainder='passthrough'

numerical_transformer = Pipeline([('log', log_transformer),('ss', ss)])
ct = ColumnTransformer([('num_transformer', numerical_transformer, skewed), ('scaler', ss, diff)], remainder='passthrough')

from sklearn.model_selection import RandomizedSearchCV
from sklearn.model_selection import GridSearchCV
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import KFold, cross_val_predict
from sklearn.metrics import mean_absolute_error
from sklearn import metrics
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix
from scipy.stats import ks_2samp
from sklearn.metrics import roc_auc_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import GradientBoostingClassifier
import xgboost as xgb

def evaluate_ks_and_roc_auc(y_real, y_proba):
    # Unite both visions to be able to filter
    df = pd.DataFrame()
    df['real'] = y_real
    df['proba'] = y_proba[:, 1]

    # Recover each class
    class0 = df[df['real'] == 0]
    class1 = df[df['real'] == 1]

    ks = ks_2samp(class0['proba'], class1['proba'])
    roc_auc = roc_auc_score(df['real'] , df['proba'])

    print(f"ROC AUC: {roc_auc:.4f}")
    print(f"KS: {ks.statistic:.4f} (p-value: {ks.pvalue:.3e})")
    return ks.statistic, roc_auc

def plot_confusion_matrix(cm,
                          target_names,
                          title='Confusion matrix',
                          cmap=None,
                          normalize=True):
    """
    given a sklearn confusion matrix (cm), make a nice plot

    Arguments
    ---------
    cm:           confusion matrix from sklearn.metrics.confusion_matrix

    target_names: given classification classes such as [0, 1, 2]
                  the class names, for example: ['high', 'medium', 'low']

    title:        the text to display at the top of the matrix

    cmap:         the gradient of the values displayed from matplotlib.pyplot.cm
                  see http://matplotlib.org/examples/color/colormaps_reference.html
                  plt.get_cmap('jet') or plt.cm.Blues

    normalize:    If False, plot the raw numbers
                  If True, plot the proportions

    Usage
    -----
    plot_confusion_matrix(cm           = cm,                  # confusion matrix created by
                                                              # sklearn.metrics.confusion_matrix
                          normalize    = True,                # show proportions
                          target_names = y_labels_vals,       # list of names of the classes
                          title        = best_estimator_name) # title of graph

    Citiation
    ---------
    http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html

    """
    import matplotlib.pyplot as plt
    import numpy as np
    import itertools

    accuracy = np.trace(cm) / np.sum(cm).astype('float')
    misclass = 1 - accuracy

    if cmap is None:
        cmap = plt.get_cmap('Blues')

    plt.figure(figsize=(8, 6))
    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.title(title)
    plt.colorbar()

    if target_names is not None:
        tick_marks = np.arange(len(target_names))
        plt.xticks(tick_marks, target_names, rotation=45)
        plt.yticks(tick_marks, target_names)

    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]


    thresh = cm.max() / 1.5 if normalize else cm.max() / 2
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        if normalize:
            plt.text(j, i, "{:0.4f}".format(cm[i, j]),
                     horizontalalignment="center",
                     color="white" if cm[i, j] > thresh else "black")
        else:
            plt.text(j, i, "{:,}".format(cm[i, j]),
                     horizontalalignment="center",
                     color="white" if cm[i, j] > thresh else "black")


    plt.tight_layout()
    plt.ylabel('True label')
    plt.xlabel('Predicted label\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))
    plt.show()

# Main pipeline for fitting.
model_LR = Pipeline([
                   ('column_transformer', ct),
                   ('smt', smt),
                   ('RF', LogisticRegression(random_state=42) )
          ])
model_LR.fit(X_train, y_train)
print("Training is success!")
y_pred = model_LR.predict_proba(X_test)
predicted = model_LR.predict(X_test)
#print AUC, KS score, and classification report
ks, auc = evaluate_ks_and_roc_auc(y_test, y_pred)
matrix = classification_report(y_test, predicted)
print('Classification report Logistic Regression : \n',matrix)
cm = confusion_matrix(y_test, predicted)
target_names = ["Bad Loan","Good Loan"]
plot_confusion_matrix(cm, target_names, title='Confusion matrix', cmap=None,normalize=False)

len(model_LR.named_steps['RF'].coef_[0])

"""## **Random Forest**"""

# Main pipeline for fitting.
model_RF = Pipeline([
                   ('column_transformer', ct),
                   ('smt', smt),
                   ('RF', RandomForestClassifier(random_state=42) )
          ])
model_RF.fit(X_train, y_train)
print("Training is success!")
y_pred = model_RF.predict_proba(X_test)
predicted = model_RF.predict(X_test)
#print AUC, KS score, and classification report
ks, auc = evaluate_ks_and_roc_auc(y_test, y_pred)
matrix = classification_report(y_test, predicted)
print('Classification report Random Forest Classifier : \n',matrix)
cm = confusion_matrix(y_test, predicted)
target_names = ["Bad Loan","Good Loan"]
plot_confusion_matrix(cm, target_names, title='Confusion matrix', cmap=None,normalize=False)

"""## **Gradient Boosting Classifier**"""

# Main pipeline for fitting.
model_GB = Pipeline([
                   ('column_transformer', ct),
                   ('smt', smt),
                   ('GB', GradientBoostingClassifier(random_state=42) )
          ])
model_GB.fit(X_train, y_train)
print("Training is success!")
y_pred = model_GB.predict_proba(X_test)
predicted = model_GB.predict(X_test)
#print AUC, KS score, and classification report
ks, auc = evaluate_ks_and_roc_auc(y_test, y_pred)
matrix = classification_report(y_test, predicted)
print('Classification report Gradient Boosting Classifier : \n',matrix)
cm = confusion_matrix(y_test, predicted)
target_names = ["Bad Loan","Good Loan"]
plot_confusion_matrix(cm, target_names, title='Confusion matrix', cmap=None,normalize=False)

"""## **XGBoost Classifier**"""

# Main pipeline for fitting.
model_XGB = Pipeline([
                   ('column_transformer', ct),
                   ('smt', smt),
                   ('XGB', xgb.XGBClassifier(objective="binary:logistic",random_state=42) )
          ])
model_XGB.fit(X_train, y_train)
print("Training is success!")
y_pred = model_XGB.predict_proba(X_test)
predicted = model_XGB.predict(X_test)
#print AUC, KS score, and classification report
ks, auc = evaluate_ks_and_roc_auc(y_test, y_pred)
matrix = classification_report(y_test, predicted)
print('Classification report XGBoost Classifier : \n',matrix)
cm = confusion_matrix(y_test, predicted)
target_names = ["Bad Loan","Good Loan"]
plot_confusion_matrix(cm, target_names, title='Confusion matrix', cmap=None,normalize=False)

"""## **Voting Classifier**"""

from sklearn.ensemble import VotingClassifier

clf1 = RandomForestClassifier(random_state=42)
clf2 = GradientBoostingClassifier(random_state=42)
clf3 = xgb.XGBClassifier(objective="binary:logistic",random_state=42)

# Main pipeline for fitting.
model_VC = Pipeline([
                   ('column_transformer', ct),
                   ('smt', smt),
                   ('VC', VotingClassifier(estimators=[('RF', clf1), ('GB', clf2), ('XGB', clf3)],
                        voting='soft', weights=[1,2,1]) )
          ])
model_VC.fit(X_train, y_train)
print("Training is success!")
y_pred = model_VC.predict_proba(X_test)
predicted = model_VC.predict(X_test)
#print AUC, KS score, and classification report
ks, auc = evaluate_ks_and_roc_auc(y_test, y_pred)
matrix = classification_report(y_test, predicted)
print('Classification report Voting Classifier : \n',matrix)
cm = confusion_matrix(y_test, predicted)
target_names = ["Bad Loan","Good Loan"]
plot_confusion_matrix(cm, target_names, title='Confusion matrix', cmap=None,normalize=False)

"""# **Model Optimization & Evaluation**
## **Hyperparameter Tuning**
"""

# Gradient Boosting
# n_jobs=-1 to allow run it on all cores
params = {
    "n_estimators":[5,50,250,500],
    "max_depth":[1,3,5,7,9],
    "learning_rate":[0.05,0.01,0,0.1]
}
# Main pipeline for fitting.
model_GB_HT = Pipeline([
                   ('column_transformer', ct),
                   ('smt', smt),
                   ('RF_HT', RandomizedSearchCV(GradientBoostingClassifier(random_state=42), params, n_jobs=-1,
                                                cv=KFold(n_splits=3), scoring='roc_auc', refit=True) )
          ])
model_GB_HT.fit(X_train, y_train)
print("Training is success!")
y_pred = model_GB_HT.predict_proba(X_test)
predicted = model_GB_HT.predict(X_test)
#print AUC, KS score, and classification report
ks, auc = evaluate_ks_and_roc_auc(y_test, y_pred)
matrix = classification_report(y_test, predicted)
print('Classification report tuned Gradient Boosting Classifier : \n',matrix)
cm = confusion_matrix(y_test, predicted)
target_names = ["Bad Loan","Good Loan"]
plot_confusion_matrix(cm, target_names, title='Confusion matrix', cmap=None,normalize=False)

model_GB_HT.steps[2][1].best_params_

# Main pipeline for fitting.
model_GB = Pipeline([
                   ('column_transformer', ct),
                   ('smt', smt),
                   ('GB', GradientBoostingClassifier(random_state=42,learning_rate=0.01,
                                                     max_depth=9,n_estimators=500) )
          ])
model_GB.fit(X_train, y_train)
print("Training is success!")
y_pred = model_GB.predict_proba(X_test)
predicted = model_GB.predict(X_test)
#print AUC, KS score, and classification report
ks, auc = evaluate_ks_and_roc_auc(y_test, y_pred)
matrix = classification_report(y_test, predicted)
print('Classification report Gradient Boosting Classifier : \n',matrix)
cm = confusion_matrix(y_test, predicted)
target_names = ["Bad Loan","Good Loan"]
plot_confusion_matrix(cm, target_names, title='Confusion matrix', cmap=None,normalize=False)

"""# **Model Checking**"""

scores_train = []
scores_test = []

lr = model_GB

kf = KFold(shuffle=True,random_state=42, n_splits=5) #random_state equals to previous train_test_split
# shuffle=True

for train_index, test_index in kf.split(df_train_feat):
  X_train = df_train_feat.take(list(train_index),axis=0)
  X_test = df_train_feat.take(list(test_index),axis=0)
  y_train, y_test = df_train_target.take(list(train_index),axis=0), df_train_target.take(list(test_index),axis=0)
  # Train the training data
  lr.fit(X_train, y_train)
  # Predict training and testing
  y_train_pred = lr.predict(X_train)
  y_pred = lr.predict(X_test)
  # Evaluating training and testing
  score_train = mean_absolute_error(y_train, y_train_pred)
  score_test = mean_absolute_error(y_test, y_pred)
  scores_train.append(score_train)
  scores_test.append(score_test)

folds = range(1, kf.get_n_splits() + 1)
plt.plot(folds, scores_train, 'o-', color='green', label='train')
plt.plot(folds, scores_test, 'o-', color='red', label='test')
plt.legend()
plt.grid()
plt.xlabel('Number of fold')
plt.ylabel('Mean Absolute Error')
plt.title('K-Fold Validation Gradient Boosting Classifier')
plt.show()

"""# **Model Interpretation**
Top 10 Feature Importances in the Model
"""

def compute_feature_importance(voting_clf, weights):
    """ Function to compute feature importance of Voting Classifier """

    feature_importance = dict()
    for est in voting_clf.estimators_:
        feature_importance[str(est)] = est.feature_importances_
    fe_scores = [0]*len(list(feature_importance.values())[0])
    for idx, imp_score in enumerate(feature_importance.values()):
        imp_score_with_weight = imp_score*weights[idx]
        fe_scores = list(np.add(fe_scores, list(imp_score_with_weight)))
    return fe_scores

feature_importance = pd.DataFrame()
feature_importance['features'] = X_train.columns
feature_importance['importance'] = model_GB.named_steps['GB'].feature_importances_

plt.figure(figsize=(30,10))
plot = feature_importance.sort_values('importance', ascending = False).head(10).plot.barh(color='blue',legend=None)
plot.set_yticklabels(feature_importance.sort_values('importance', ascending = False).head(10).features)
plt.title('Feature Importance in Gradient Boosting Model')
plt.xlim([0,0.7])
plt.xlabel('Importance')
plt.ylabel('Feature')
plt.gca().invert_yaxis()
plt.show()